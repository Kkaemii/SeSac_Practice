{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 I / O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'카메라를 홍보하기 위한 좋은 문구를 추천해줘'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"{product}를 홍보하기 위한 좋은 문구를 추천해줘\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"product\"], template=template)\n",
    "\n",
    "prompt.format(product=\"카메라\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "강아지입니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm1 = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "\n",
    "prompt = \"진희는 강아지를 키우고 있습니다. 진희가 키우고 있는 동물은?\"\n",
    "\n",
    "print(llm1.invoke(prompt).content)\n",
    "\n",
    "\n",
    "# invoke는 content 속성에 답변을 담고있고,\n",
    "# 토큰 사용량, 모델 이름등의 메타데이터와 기타 추가정보를 담고있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\seSac_env\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "진희는 강아지를 키우고 있습니다. 진희가 키우고 있는 동물은?\n",
      "미물은 키우고 있는 미물은?\n",
      "미물은 키우고 있는 미물은?\n",
      "미물은 키우고 있는 미물은?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hug_api = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "llm2 = HuggingFaceHub(\n",
    "    repo_id=\"tiiuae/falcon-7b-instruct\", model_kwargs={\"max_new_tokens\": 100}\n",
    ")\n",
    "\n",
    "prompt = \"진희는 강아지를 키우고 있습니다. 진희가 키우고 있는 동물은?\"\n",
    "completion = llm2.invoke(prompt, max_new_tokens=100, stream=False)\n",
    "\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModelLaboratory로 LLM모델 생성 결과 비교하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.model_laboratory import ModelLaboratory\n",
    "\n",
    "model_lab = ModelLaboratory.from_llms([llm1, llm2])\n",
    "model_lab.compare(\"대한민국의 가을은 몇 월 부터 몇 월 까지야?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output_Parser를 통해 출력 결과 포멧 지정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='두산 베어스, LG 트윈스, 키움 히어로즈, SSG 랜더스, NC 다이노스, 롯데 자이언츠, 삼성 라이온즈' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 48, 'total_tokens': 92, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_6dd05565ef', 'finish_reason': 'stop', 'logprobs': None} id='run-658a956c-1d7e-482c-9da9-43b1115ac735-0'\n",
      "==================================================================================================== \n",
      "\n",
      "['두산 베어스', 'LG 트윈스', '키움 히어로즈', 'SSG 랜더스', 'NC 다이노스', '롯데 자이언츠', '삼성 라이온즈']\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, max_tokens=2048, model=\"gpt-4o\")\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()  # 파서 초기화\n",
    "format_instructions = output_parser.get_format_instructions()  # 출력 형식 지정\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"7개의 팀을 보여줘 {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "query = \"한국의 야구팀은?\"\n",
    "\n",
    "# 출력 결과 생성\n",
    "output = llm.invoke(input=prompt.format(subject=query))\n",
    "print(output)\n",
    "print(\"==\" * 50, \"\\n\")\n",
    "# 출력에 대한 포멧 변경\n",
    "parsered_result = output_parser.parse(output.content)\n",
    "print(parsered_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 연결 (PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1    The Fence \\n \\nTom Sawyer lived with his aunt because his mother and \\nfather were dead. Tom didn’t like going to school, and he \\ndidn’t like working. He liked playing and having \\nadventures. One Friday, he didn’t go to school—he went \\nto the river. \\nAunt Polly was angry. “You’re a bad boy!” she said. \\n“Tomorrow you can’t play with your friends because you \\ndidn’t go to school today. Tomorrow you’re going to work \\nfor me. You can paint the fence.” \\nSaturday morning, Tom was not happy, but he started to \\npaint the fence. His friend Jim was in the street. \\nTom asked him, “Do you want to paint?” \\nJim said, “No, I can’t. I’m going to get water.” \\nThen Ben came to Tom’s house. He watched Tom and \\nsaid, “I’m going to swim today. You can’t swim because \\nyou’re working.” \\nTom said, “This isn’t work. I like painting.” \\n“Can I paint, too?” Ben asked. \\n“No, you can’t,” Tom answered. “Aunt Polly asked me \\nbecause I’m a very good painter.” \\nBen said, “I’m a good painter, too. Please, can I paint? I \\nhave some fruit. Do you want it?” \\nOK,” Tom said. “Give me the fruit. Then you can paint.” \\nBen started to paint the fence. Later, many boys came to \\nTom’s house. They watched Ben, and they wanted to \\npaint, too. \\nTom said, “Give me some food and you can paint.” \\n \\n1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./The_Adventures_of_Tom_Sawyer.pdf\")\n",
    "document = loader.load()\n",
    "document[5].page_content[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12168\\3427618908.py:10: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()  # 임베딩 모듈 초기화\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()  # 임베딩 모듈 초기화\n",
    "db = FAISS.from_documents(document, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0028138665231909497, -0.020238374776143958, -0.012738939981790699, -0.016539159741274387, -0.02259930910767389, 0.028709960370719174, -0.02787668932061184, 0.0063410627904474165, -0.012025609306402632, 0.009797242220737239]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "text = \"진희는 강아지를 키우고 있습니다. 진희가 키우고 있는 동물은?\"\n",
    "text_embedding = embeddings.embed_query(text)\n",
    "print(text_embedding[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12168\\3798662067.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.013278793543577194, 0.07225918024778366, 0.0926310271024704, -0.00397957768291235, 0.0015618291217833757]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "text = \"진희는 강아지를 키우고 있습니다. 진희가 키우고 있는 동물은?\"\n",
    "embedded_text = embeddings.embed_query(text)\n",
    "print(embedded_text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "마을 무덤에서 의사를 죽인 사람은 인디언 조예요. 그는 칼로 의사를 죽였어요.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "query = \"마을 무덤에 있던 남자를 죽인 사람은 누구고 어떻게 죽였어?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'캐나다의 수도는 오타와입니다.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "prompt = PromptTemplate(\n",
    "    input_varidables=[\"country\"], template=\"{country}의 수도는 어디야?\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)  # 프롬프트와 모델을 체인으로 연결\n",
    "chain.run(\"캐나다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': '\\nOne limitation of LLMs is their lack of contextual information (e.g., access to some specific documents or emails). You can combat this by giving LLMs access to the specific external data.\\nFor this, you first need to load the external data with a document loader. LangChain provides a variety of loaders for different types of documents ranging from PDFs and emails to websites and YouTube videos.\\n',\n",
       " 'stranslation': 'LLM의 한 가지 제한점은 특정 문서나 이메일에 대한 접근과 같은 맥락 정보가 부족하다는 것입니다. 이를 해결하기 위해 LLM에 특정 외부 데이터에 대한 접근 권한을 부여할 수 있습니다. 이를 위해 먼저 문서 로더를 사용하여 외부 데이터를 로드해야 합니다. LangChain은 PDF, 이메일, 웹사이트, YouTube 동영상 등 다양한 유형의 문서에 대한 다양한 로더를 제공합니다.',\n",
       " 'summary': 'LLM의 맥락 정보 부족 문제를 해결하기 위해 LangChain은 PDF, 이메일, 웹사이트, YouTube 동영상 등 다양한 외부 데이터를 로드할 수 있는 문서 로더를 제공합니다.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 프롬프트 1 정의\n",
    "prompt1 = PromptTemplate(\n",
    "    input_variabels=[\"sentence\"],\n",
    "    template=\"다음 문장을 한글로 번역하세요 \\n\\n {sentence}\",\n",
    ")\n",
    "\n",
    "# 번역체인 1에 대한 모델\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt1, output_key=\"stranslation\")\n",
    "\n",
    "# 프롬프트 2 정의\n",
    "prompt2 = PromptTemplate.from_template(\n",
    "    \"다음 문장을 한 문장으로 요약하세요. \\n\\n {stranslation}\"\n",
    ")\n",
    "\n",
    "# 요약체인 2 에 대한 모델\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt2, output_key=\"summary\")\n",
    "\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "all_chain = SequentialChain(\n",
    "    chains=[chain1, chain2],\n",
    "    input_variables=[\"sentence\"],\n",
    "    output_variables=[\"stranslation\", \"summary\"],\n",
    ")\n",
    "\n",
    "sentence = \"\"\"\n",
    "One limitation of LLMs is their lack of contextual information (e.g., access to some specific documents or emails). You can combat this by giving LLMs access to the specific external data.\n",
    "For this, you first need to load the external data with a document loader. LangChain provides a variety of loaders for different types of documents ranging from PDFs and emails to websites and YouTube videos.\n",
    "\"\"\"\n",
    "all_chain(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 메모리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12168\\2768668027.py:5: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(llm=llm, verbose=True)\n",
      "c:\\ProgramData\\anaconda3\\envs\\seSac_env\\lib\\site-packages\\pydantic\\main.py:214: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 진희는 강아지를 한마리 키우고 있습니다.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 진희는 강아지를 한마리 키우고 있습니다.\n",
      "AI: 진희 씨는 강아지를 키우고 있군요! 강아지의 이름이 무엇인가요?\n",
      "Human: 영수는 고양이를 두마리 키우고 있습니다.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 진희는 강아지를 한마리 키우고 있습니다.\n",
      "AI: 진희 씨는 강아지를 키우고 있군요! 강아지의 이름이 무엇인가요?\n",
      "Human: 영수는 고양이를 두마리 키우고 있습니다.\n",
      "AI: 영수 씨는 고양이를 두 마리 키우고 있군요! 고양이들의 이름이 무엇인가요?\n",
      "Human: 진희와 영수가 키우는 동물은 총 몇마리?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'진희 씨는 강아지 한 마리를 키우고, 영수 씨는 고양이 두 마리를 키우고 있으니, 총 세 마리의 동물을 키우고 있습니다.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, name=\"gpt-4o\")\n",
    "\n",
    "from langchain import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "conversation.predict(input=\"진희는 강아지를 한마리 키우고 있습니다.\")\n",
    "conversation.predict(input=\"영수는 고양이를 두마리 키우고 있습니다.\")\n",
    "conversation.predict(input=\"진희와 영수가 키우는 동물은 총 몇마리?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12168\\1331107071.py:7: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo answer the question, I need to find out the birth year of Jang Wonyoung and then calculate her age in 2025.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Jang Wonyoung\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Jang Won-young\n",
      "Summary: Jang Won-young (Korean: 장원영; born August 31, 2004), also known mononymously as Wonyoung, is a South Korean singer. She is a member of the South Korean girl group Ive under Starship Entertainment and former member of project girl group Iz*One, having finished first in Mnet's girl group survival reality television show Produce 48 in 2018.\n",
      "\n",
      "Page: Sunghoon (singer, born 2002)\n",
      "Summary: Park Sung-hoon (Korean: 박성훈; born December 8, 2002), known mononymously as Sunghoon, is a South Korean singer and former figure skater. He competed as a figure skater from 2010 to early 2020; while simultaneously being a K-pop trainee since 2018. He retired from the sport and debuted as a member of the South Korean boy band Enhypen in November 2020. Sunghoon is the 2016–2017 junior silver medalist and the 2015 novice gold medalist of Asian Figure Skating Trophy, and the 2015 novice gold medalist of Lombardia Trophy. He also won silver medals at the 2013 novice competition and the 2014 junior competition of South Korean Figure Skating Championships.\n",
      "\n",
      "Page: List of Produce 48 contestants\n",
      "Summary: Produce 48 is a 2018 South Korean competition television show on Mnet in which a girl group is formed from a large pool of South Korean and Japanese idols, most of whom are represented by major idol talent agencies such as AKB48 Group. The show starts with 96 contestants, with occasional rounds of competition and voting, some of which result in elimination from the contest. Of the 96 contestants, 57 of them are from South Korean management companies, and 39 of them are from Japanese idol groups, composed of current members of AKB48 and their sister groups. The final 12 contestants, with no nationality caps, as voted only by Korean viewers, will promote as a group for two and a half years.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mJang Wonyoung was born on August 31, 2004. To calculate her age in 2025, we subtract her birth year from 2025.\n",
      "\n",
      "Action: Calculator\n",
      "Action Input: 2025 - 2004\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 21\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer. Jang Wonyoung was born in 2004, and in 2025, she will be 21 years old. \n",
      "\n",
      "Final Answer: Jang Wonyoung was born in 2004, and she will be 21 years old in 2025.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Jang Wonyoung was born in 2004, and she will be 21 years old in 2025.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "\n",
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    description=\"계산이 필요할 때 사용\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "agent.run(\"아이브 장원영이 태어난 해는? 2025년 현재의 나이는?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seSac_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
